{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bndP0M34i_6N"
   },
   "source": [
    "# Домашнее задание 3\n",
    "\n",
    "__Дедлайн: 29.11.2021, 23:59__\n",
    "\n",
    "Файлы должны иметь имя HW1_ФАМИЛИЯ.ipynb, где ФАМИЛИЯ - Ваша фамилия русскими буквами.\n",
    "\n",
    "Решение каждой задачи необходимо поместить после её условия.\n",
    "\n",
    "Пожалуйста, пишите свои решения чётко и понятно.\n",
    "При полном запуске Вашего решения (Kernel -> Restart & Run All) все ячейки должны выполняться **без ошибок**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l9suLK9i_6T"
   },
   "source": [
    "Во всех задачах, где требуется сравнить методы, требуется аккуратное офрмление графиков, если Вы их будете строить, и комментарии к графикам (кто кого быстрее и когда сходится, какие интересные моменты Вы видите на графиках, как Вы их можете объяснить). Аналогично, требуются комментарии в случае, если Вы сравниваете время работы.\n",
    "\n",
    "Задачи со звездочкой не являются обязательными к решению, но за них будут даны бонусные баллы.\n",
    "\n",
    "Суммарное количество баллов: 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE1furDSi_6V"
   },
   "source": [
    "## Задача 1. Wolfe Rule [4]\n",
    "\n",
    "Далее мы рассматриваем метод вида $x_{k+1}=x_k+\\alpha_k h_k$, где $\\alpha_k>0$-шаг метода, $h_k$ - направление убывания.\n",
    "\n",
    "1) [2] Пусть функция $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ является дважды дифференцируемой функцией. Покажите, что условие кривизны $\\langle \\nabla f(x_{k+1}), h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle, \\beta_2\\in(0,1)$ на шаг $\\alpha$ не выполняется для достаточно малого $\\alpha$.\n",
    "\n",
    "2) [2] Пусть функция $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ является непрерывно дифференцируемой функцией, и в правиле Вольфа константы $\\beta_1,\\beta_2$ выбраны так, что $0<\\beta_1<\\beta_2<1$. Пусть также нам известно, что уравнение вида $f(x_k+\\alpha_k h_k) = f(x_k) + \\alpha \\beta_1 \\langle \\nabla f(x_k), h_k\\rangle$  имеет хотя бы одно нетривиальное решение. Пусть $\\alpha'\\neq 0$ - минимальное из этих решений. Покажите, что тогда найдётся точка $\\alpha'' \\in (0,\\alpha')$, которая удовлетворяет шагу Вольфа.\n",
    "\n",
    "*Hint:* В этой задаче может быть полезна формула Тейлора и связанные с ней утверждения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwOMA4LKi_6W"
   },
   "source": [
    "### Решение задачи 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLhRb4Xwi_6Y"
   },
   "source": [
    "1) $$\\langle \\nabla f(x_{k+1}), h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle,$$\n",
    "$$\\langle \\nabla f(x_{k} + \\alpha_k h_k), h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle,$$\n",
    "\n",
    "распишем градиент в точке $x_{k+1}$ \n",
    "\n",
    "$$\\nabla f(x_k + \\alpha_k h_k) \\approx \\nabla f(x_k) + \\alpha_k \\nabla^2 f(x_k) h_k,$$\n",
    "\n",
    "подставляем\n",
    "\n",
    "$$\\langle \\nabla f(x_k) + \\alpha_k \\nabla^2 f(x_k) h_k, h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle,$$\n",
    "\n",
    "применяем свойство скалярного произведения\n",
    "\n",
    "$$\\langle \\nabla f(x_k), h_k \\rangle + \\alpha_k \\langle \\nabla^2 f(x_k) h_k, h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle,$$\n",
    "\n",
    "$$ \\langle \\nabla^2 f(x_k) h_k, h_k\\rangle \\geq (\\beta_2 - 1) \\langle \\nabla f(x_k), h_k \\rangle,$$\n",
    "\n",
    "т.к. $h_k$ - направление убывания, а $\\beta_2\\in(0,1)$, то правая часть точно положительная при условии, что $h_k \\neq 0$, в то время как левая часть может быть либо отрицательной, либо выбрать $\\alpha_k$ такой, чтоб неравенство невыполнялось.\n",
    "\n",
    "\n",
    "2) Рассмотрим условия Вольфа для шага $\\alpha$:\n",
    "- условие Армихо (достаточное убывание):\n",
    "$$f(x_k+\\alpha h_k) \\leq f(x_k) + \\alpha \\beta_1\\langle \\nabla f(x_k), h_k \\rangle, $$\n",
    "- условие кривизны\n",
    "$$\\langle \\nabla f(x_k + \\alpha h_k), h_k\\rangle \\geq \\beta_2 \\langle \\nabla f(x_{k}), h_k\\rangle,$$\n",
    "Рассмотрим функцию\n",
    "$$ g(\\alpha) = f(x_k+\\alpha h_k) - f(x_k) - \\alpha \\beta_1\\langle \\nabla f(x_k), h_k \\rangle, $$\n",
    "$g(0) = 0$ и $g(\\alpha ') \\leq 0$ - из условия Армихо\n",
    "\n",
    "Но $g(\\alpha ') = 0$, т.к. $\\alpha '$ - решение. \n",
    "\n",
    "$g(0) = 0$ и $g(\\alpha ') = 0$, значит, по теореме Ролля существует $\\alpha''\\in (0,\\alpha')$, такое, что $g'(\\alpha'') = 0$,\n",
    "\n",
    "где выражение для $g'(\\alpha)$ выглядит следующим образом:\n",
    "\n",
    "$$g'(\\alpha)=\\langle \\nabla f(x_k+\\alpha h_k), h_k \\rangle - \\beta_1 \\langle \\nabla f(x_k), h_k \\rangle $$\n",
    "\n",
    "Значит, если $g'(\\alpha'') = 0$, то\n",
    "\n",
    "$$\\langle \\nabla f(x_k+\\alpha h_k), h_k \\rangle = \\beta_1 \\langle \\nabla f(x_k), h_k \\rangle $$, что как раз удовлетворяет условию кривизны. \n",
    "\n",
    "Таким образом, найденное значение удовлетворяет и условию Армихо (достаточное убывание), и условию кривизны, и поэтому является допустимым шагом Вольфа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rsi63uPZi_6Z"
   },
   "source": [
    "## Задача 2. Свойства гладких функций [6]\n",
    "\n",
    "Докажите следующее утверждения про константу Липшица градиента:\n",
    "\n",
    "1. [1] Пусть для любого $i=\\overline{1,m}$ функция $f_i$ является $L_i$-гладкой. Тогда $f(x)=\\sum\\limits_{i=1}^m f_i(x)$ тоже $L$-гладкая с константой $L=\\sum\\limits_{i=1}^m  L_i$.\n",
    "\n",
    "2. [2] Пусть $f(y)$ - $L$-гладкая функция. Покажите, что $g(x)=f(Ax+b)$ есть $M$-гладкая функция с константой $M=\\|A\\|_2^2 L$\n",
    "\n",
    "3. [3] Пусть $f(x)$ есть всюду дважды дифференцируемая функция и $\\lambda_{\\max}(\\nabla^2 f(x))\\leq L, \\forall x\\in \\mathbb{R}^n$. Тогда $f(x)$ - есть $L$-гладкая функция.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjl5Y2KYi_6a"
   },
   "source": [
    "1. Функция $f_i$ является $L_i$-гладкой, значит для любого $i=\\overline{1,m}$ верно \n",
    "$$|| \\nabla f_i(x) - \\nabla f_i(y)|| \\leq L_i ||x-y||.$$ \n",
    "Но тогда то же неравенство выполняется и для суммы по i: \n",
    "$$\\sum\\limits_{i=1}^m|| \\nabla f_i(x) - \\nabla f_i(y)|| \\leq \\sum\\limits_{i=1}^mL_i ||x-y||.$$\n",
    "Применим неравенство треугольника и получаем:\n",
    "$$||\\nabla\\sum\\limits_{i=1}^m f_i(x) - \\nabla\\sum\\limits_{i=1}^m f_i(y)|| \\leq \\sum\\limits_{i=1}^m|| \\nabla f_i(x) - \\nabla f_i(y)|| \\leq \\sum\\limits_{i=1}^mL_i ||x-y||.$$\n",
    "\n",
    "Тогда $f(x)=\\sum\\limits_{i=1}^m f_i(x)$ $L$-гладкая с константой $L=\\sum\\limits_{i=1}^m  L_i$, ч.т.д.\n",
    "\n",
    "2. Пусть $Ax+b = x', Ay+b=y'$, тогда $\\nabla g(x) = A^T \\nabla f(x')$. Где $\\nabla f(x')$ - значение градиента в точке Ax+b. Тогда\n",
    "$$||\\nabla g(x) - \\nabla g(y)|| = ||A^T(\\nabla f(x') - \\nabla f(y'))||$$\n",
    "$$||\\nabla g(x) - \\nabla g(y)|| \\leq ||A^T||||(\\nabla f(x') - \\nabla f(y'))||$$\n",
    "По условиям исходная функция L-гладкая, значит:\n",
    "$$||\\nabla g(x) - \\nabla g(y)|| \\leq ||A^T||||(\\nabla f(x') - \\nabla f(y'))|| \\leq $$ $$ \\leq||A^T||L||x'-y'|| \\leq ||A^T||||(\\nabla f(x') - \\nabla f(y'))|| \\leq ||A^T||L||Ax - Ay|| \\leq ||A^2||L||x - y||.$$\n",
    "То есть $g(x)$ - $M$-гладкая функция с константой $M=\\|A\\|_2^2 L$, ч.т.д.\n",
    "\n",
    "3.Используем формулу Тейлора для функции, используя значения в точках x и y:\n",
    "$$f(y)=f(x)+\\nabla f^T(x)(y-x).$$\n",
    "Теперь рассмотрим градиенты от обеих сторон уравнения\n",
    "$$\\nabla f(y) = \\nabla f(x) +\\nabla^2 f(x)(y-x)$$\n",
    "Теперь возьмём норму от обеих частей\n",
    "$$||\\nabla f(y) - \\nabla f(x)|| = ||\\nabla^2 f(x)(y-x)||$$\n",
    "Используем оценку сверху для нормы матрицы:\n",
    "$$||\\nabla^2 f(x)(y-x)|| \\leq \\lambda_max(\\nabla^2 f(x)) ||y-x||)$$\n",
    "Учитывая, что $\\lambda_{\\max}(\\nabla^2 f(x))\\leq L$, мы получаем:\n",
    "$$||\\nabla f(y) - \\nabla f(x)|| \\leq L||y-x||,$$\n",
    "ч.т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6ZLKnWzi_6b"
   },
   "source": [
    "## Задача 3. Линейная регрессия [9]\n",
    "\n",
    "Рассмотрим задачу линейной регресии с $\\ell_2$ регуляризатором:\n",
    "\n",
    "$$\\min_{w\\in\\mathbb{R}^n} \\|Xw-y\\|^2 + \\frac{\\lambda}{2}\\|w\\|^2,$$\n",
    "где $X\\in\\mathbb{R}^{m\\times n}, y\\in\\mathbb{R}^m, \\lambda \\in \\mathbb{R}_+$ - параметры задачи. В данном упражнении мы изучим эффективность метода сопряженных градиентов в сравнении с прямыми методами в зависимости от структуры данных $X$. Рассмотрим три вида данных:\n",
    "\n",
    "* $X$ не имеет специальной структуры, $m > n$ (файл `data/task3/X.npy`)\n",
    "* $X$ есть разреженная матрица, $m=n$ (файл `data/task3/X_sparse.npy`)\n",
    "* $X$ не имеет специальной структуры, но $m \\ll n$ (файл `data/task3/X_low_rank.npy`)\n",
    "\n",
    "Таргеты $y$ берутся из файлов `data/task3/y*.npy` с соответсвующим именем. Коэффициент $\\lambda$ во всех случаях возьмем равным $10^{-1}$.\n",
    "\n",
    "1. [2] Выпишите явно решение для задачи линейной регресии. Реализуйте функцию, которая принимает на вход $X$, $y$ и $w$ возвращает решение (рекомендуется использовать функцию `np.linalg.solve`, а не `np.linalg.inv` в этом задании)\n",
    "\n",
    "2. [1] Сравните два способа вычисления решения по времени:\n",
    "    * Явное вычисление решение методом из пункта 1\n",
    "    * Использование метода сопряженных градиентов с критерием останова $\\|\\nabla f(x)\\|\\leq \\varepsilon=10^{-6}$.\n",
    "\n",
    "3. [2] Реализуйте эффективно метод сопряженных градиентов для разреженной матрицы. Проведите такое же сравнение, как в пункте 2.\n",
    "\n",
    "4. [3] Реализуйте эффективно метод сопряженных градиентов для матрицы без структуры, но с $m \\ll n$. Проведите такое же сравнение, как в пункте 2.\n",
    "\n",
    "5. [1] Исходя из того, что для точки выхода верно $\\|\\nabla f(x)\\|\\leq \\varepsilon=10^{-6}$, дайте оценку на величину $f(x)-f^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlaxxD-Ri_6c"
   },
   "source": [
    "### Решение задачи 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cam8B_ESi_6e"
   },
   "outputs": [],
   "source": [
    "# Место для Вашего решения\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.optimize import minimize\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.optimize import fmin_cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Явное вычисление: 0.569129467010498 сек.\n",
      "Метод сопряженных градиентов: 617.6804668903351 сек.\n"
     ]
    }
   ],
   "source": [
    "# Первый пункт\n",
    "\n",
    "def linear_regression_solution(X, Y, lambda_val):\n",
    "    # Решение задачи линейной регрессии с l2-регуляризацией\n",
    "    # X: матрица признаков (m x n)\n",
    "    # Y: вектор целевых значений (m,)\n",
    "    # lambda_val: параметр регуляризации\n",
    "\n",
    "    # Размерности матрицы X\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Вычисление решения\n",
    "    w = np.linalg.solve(X.T @ X + lambda_val * np.identity(n), X.T @ Y)\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# Загрузка данных\n",
    "X = np.load('X.npy')\n",
    "n = 3000\n",
    "Y = np.load('y.npy')\n",
    "lambda_val = 0.05\n",
    "\n",
    "# Явное вычисление решения\n",
    "start_time = time.time()\n",
    "w_explicit = linear_regression_solution(X, Y, lambda_val)\n",
    "explicit_time = time.time() - start_time\n",
    "\n",
    "# Второй пункт\n",
    "\n",
    "# Метод сопряженных градиентов\n",
    "\n",
    "def linear_regression_loss(w, X, y, lambda_val):\n",
    "    # Функция потерь\n",
    "    loss = np.linalg.norm(X @ w - y)**2 + 0.5 * lambda_val * np.linalg.norm(w)**2\n",
    "    \n",
    "    # Градиент функции потерь\n",
    "    gradient = 2 * X.T @ (X @ w - y) + lambda_val * w\n",
    "    \n",
    "    return loss, gradient\n",
    "\n",
    "n = X.shape[1]\n",
    "w0 = np.zeros(n)\n",
    "\n",
    "start_time = time.time()\n",
    "result = minimize(fun=lambda w: linear_regression_loss(w, X, Y, lambda_val),\n",
    "                  x0=w0,\n",
    "                  method='CG',  # Выбор метода сопряженных градиентов\n",
    "                  jac=True,  # Указание, что предоставлен градиент\n",
    "                  tol=1e-6)  # Критерий останова\n",
    "                  \n",
    "cg_time = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Явное вычисление: {explicit_time} сек.\")\n",
    "print(f\"Метод сопряженных градиентов: {cg_time} сек.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1343.756253\n",
      "         Iterations: 307\n",
      "         Function evaluations: 541\n",
      "         Gradient evaluations: 541\n",
      "Явное вычисление: 0.59625244140625 сек.\n",
      "Метод сопряженных градиентов (разреженная матрица): 4.529702425003052 сек.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def linear_regression_solution(X, y, lambda_val):\n",
    "    n = X.shape[1]\n",
    "    return np.linalg.solve(X.T @ X + lambda_val * np.identity(n), X.T @ y)\n",
    "\n",
    "def cg_sparse(X, y, lambda_val):\n",
    "    A = X.T @ X + lambda_val * np.identity(X.shape[1])\n",
    "    b = X.T @ y\n",
    "    w0 = np.zeros_like(b)\n",
    "    \n",
    "    def objective_function(w):\n",
    "        return 0.5 * (w @ A @ w) - b @ w\n",
    "    \n",
    "    def gradient(w):\n",
    "        return A @ w - b\n",
    "    \n",
    "    result = fmin_cg(objective_function, w0, fprime=gradient, gtol=1e-6)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Загрузка данных\n",
    "X_sparse = np.load('X_sparse.npy')\n",
    "Y_sparse = np.load('y_sparse.npy')\n",
    "lambda_val = 1e-1\n",
    "\n",
    "# Явное вычисление решения\n",
    "start_time = time.time()\n",
    "w_explicit = linear_regression_solution(X_sparse, Y_sparse, lambda_val)\n",
    "explicit_time = time.time() - start_time\n",
    "\n",
    "# Метод сопряженных градиентов для разреженной матрицы\n",
    "start_time = time.time()\n",
    "w_cg_sparse = cg_sparse(X_sparse, Y_sparse, lambda_val)\n",
    "cg_sparse_time = time.time() - start_time\n",
    "\n",
    "print(f\"Явное вычисление: {explicit_time} сек.\")\n",
    "print(f\"Метод сопряженных градиентов (разреженная матрица): {cg_sparse_time} сек.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Явное вычисление: 0.39598965644836426 сек.\n",
      "Метод сопряженных градиентов: 0.2872333526611328 сек.\n"
     ]
    }
   ],
   "source": [
    "# Четвёртый пункт\n",
    "X_low_rank = np.load('X_low_rank.npy')\n",
    "Y_low_rank = np.load('y_low_rank.npy')\n",
    "\n",
    "def linear_regression_loss(w, X, y, lambda_val):\n",
    "    loss = np.linalg.norm(X @ w - y)**2 + 0.5 * lambda_val * np.linalg.norm(w)**2\n",
    "    gradient = 2 * X.T @ (X @ w - y) + lambda_val * w\n",
    "    \n",
    "    return loss, gradient\n",
    "\n",
    "n = X.shape[1]\n",
    "w0 = np.zeros(n)\n",
    "\n",
    "\n",
    "# Явное вычисление решения\n",
    "start_time = time.time()\n",
    "w_explicit = linear_regression_solution(X_low_rank, Y_low_rank, lambda_val)\n",
    "explicit_time = time.time() - start_time\n",
    "\n",
    "# Метод сопряженных градиентов для матрицы без специальной структуры\n",
    "start_time = time.time()\n",
    "result = minimize(fun=lambda w: linear_regression_loss(w, X_low_rank, Y_low_rank, lambda_val),\n",
    "                  x0=w0,\n",
    "                  method='CG',  # Выбор метода сопряженных градиентов\n",
    "                  jac=True,  # Указание, что предоставлен градиент\n",
    "                  tol=1e-6)  # Критерий останова\n",
    "low_rank_time = time.time() - start_time\n",
    "\n",
    "print(f\"Явное вычисление: {explicit_time} сек.\")\n",
    "print(f\"Метод сопряженных градиентов: {low_rank_time} сек.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJSqiExKi_6f"
   },
   "source": [
    "## Задача 4. Логистическая регрессия [25]\n",
    "\n",
    "Рассмотрим задачу логистической регрессии:\n",
    "\n",
    "$$\\min_{w\\in\\mathbb{R}^n} \\frac{1}{m} \\sum\\limits_{i=1}^m \\log\\left(1+\\exp(-y_i \\langle w, x_i\\rangle)\\right) + \\frac{\\alpha}{2}\\|w\\|_2^2,$$\n",
    "где $\\{x_i, y_i\\}_{i=1}^m \\in \\mathbb{R}^n\\times \\{-1,1\\}$ есть обучающая выборка, $\\alpha\\in\\mathbb{R}_+$ - некоторый параметр $\\ell_2$-регуляризации. Параметр $\\alpha=0.01$ в этой задаче, матрицы $X,y$ загрузите из файлов `data/task4/X.npy` и `data/task4/y.npy.`\n",
    "\n",
    "В этом задании мы сравним различные методы для решения этой задачи. В качестве критерия останова и измерения качества мы будем использовать зазор двойственности. Заметим, что для задачи выше построить двойственную нельзя, в силу отсутсвия ограничений. Однако в таком случае можно переписать задачу выше введя новые переменные и ограничения\n",
    "$$\\min_{u\\in\\mathbb{R^m}, w\\in\\mathbb{R}^n} \\frac{1}{m} \\sum\\limits_{i=1}^m \\log\\left(1+\\exp(-u_i)\\right) + \\frac{\\alpha}{2}\\|w\\|_2^2,$$\n",
    "$$\\text{s.t. } u_i=y_i \\langle w, x_i\\rangle, \\forall i=\\overline{1,m}.$$\n",
    "Заметим, что для этой задачи выполняются условия Слейтера.\n",
    "\n",
    "0. [1] Реализуйте вычисление целевой функции и её градиента. Проверьте, что ваши реализации являются устойчивыми и дают адекватный результат при больших значениях $|y_i \\langle w, x_i\\rangle|$.\n",
    "\n",
    "1. [3] Постройте двойственную задачу для задачи выше.\n",
    "\n",
    "2. [2] Получите в явном виде выражение оптимального значения двойственной переменной $\\nu(w^*)$ через оптимальной значение исходной $w^*.$\n",
    "\n",
    "3. [2] Пусть $\\nu(w)$ - выражение двойственной переменной через исходную, исходя из предыдущего пункта, $f(w)$ - оптимизируемая функция в исходной задаче, $d(\\nu)$ - в двойственной. Тогда определим зазор двойственности на $k$-ой итерации, как $gap(w_k)=f(w_k)-d(\\nu(w_k)).$ Далее в качестве критерия останова, а также измерения скорости сходимости будет использован именно зазор двойственности.\n",
    "\n",
    "4. [1] Чему равна константа сильной выпуклости $\\mu$ исходной задачи?\n",
    "\n",
    "5. [2] Пользуясь утверждениями из Задачи 2 найдите константу Липшица градиента $L$ исходной задачи.\n",
    "\n",
    "6. [2] Реализуйте поиск шага, удовлетворяющего условиям Гольдштейна\n",
    "\n",
    "7. [1] Сравните градиентный спуск с различными адаптивными стратегиями (Армихо, Вольф, Гольдштейн) и выберите наилучший. Используйте параметры $\\rho=\\frac{1}{2}, \\beta_1=0.3, \\beta_2=0.9$ или бликие к ним.\n",
    "\n",
    "8. [2] Реализуйте метод Хестенса-Штифеля (HS) с рестартами\n",
    "\n",
    "9. [2] Добавьте во все методы сопряженных градиентов возможность использования условия рестартов, основанное на коллинеарности градиентов на соседних итерациях\n",
    "\n",
    "10. [1] Подберите как можно лучшее условие рестарта для каждого из методов FR, PR и HS. Шаг выбирайте при помощи правила Вольфа\n",
    "\n",
    "11. [3] Сравните по итерациям и по времени следующие методы\n",
    "\n",
    "    * Градиентный спуск с лучшей адаптивной стратегией выбора шага (пункт 3)\n",
    "\n",
    "    * Градиентный спуск с шагом $\\frac{1}{L}$\n",
    "    \n",
    "    * Метод Нестерова с оптимальным шагом\n",
    "\n",
    "    * Метод тяжелого шарика с оптимальным шагом\n",
    "\n",
    "    * Методы FR, PR и HS с лучшими условиями рестарта с предыдущего пункта\n",
    "    \n",
    "   Сделайте выводы.\n",
    "   \n",
    "12. [3] В этом задании мы использовали в качестве меры качества зазор двойственности, который мажорирует качество решения по функции $f(w_k)-f_* \\leq f(w_k)-d(\\nu(w_k))$. Но мы можем построить и другую оценку сверху. В силу того, что функция $f$ является сильно выпуклой $f(w_k)-f_*$ можно также оценить, как $f(w_k)-f_* \\leq \\frac{\\|\\nabla f(w_k)\\|^2}{2\\mu}$. Выберете лучший метод с прошлой задачи и сравните эти две верхние оценки для последовательности точек $\\{w_k\\}_{k=1}^N.$ Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx0hgT4Hi_6h"
   },
   "source": [
    "### Решение задачи 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sM0m3b-bi_6h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Loss: 0.7487147789254397\n",
      "Gradient: [-0.07270128 -0.01886731 -0.09669113]\n"
     ]
    }
   ],
   "source": [
    "# Место для Вашего решения\n",
    "# №0\n",
    "\n",
    "def logistic_loss(w, X, y, alpha):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Вычисление отклика модели\n",
    "    z = y * np.dot(X, w)\n",
    "    \n",
    "    # Защита от переполнения\n",
    "    z_safe = np.maximum(0, -z)\n",
    "    \n",
    "    # Вычисление целевой функции (logistic loss) и её градиента\n",
    "    loss = np.mean(np.log(1 + np.exp(z_safe))) + 0.5 * alpha * np.sum(w**2)\n",
    "    gradient = -np.dot(X.T, (y * np.exp(-z_safe)) / (1 + np.exp(-z_safe))) / m + alpha * w\n",
    "    \n",
    "    return loss, gradient\n",
    "\n",
    "# Тестирование функции\n",
    "n = 3  # Количество признаков\n",
    "m = 5  # Количество примеров\n",
    "\n",
    "np.random.seed(42)\n",
    "X_test = np.random.rand(m, n)\n",
    "y_test = np.random.choice([-1, 1], m)\n",
    "w_test = np.random.rand(n)\n",
    "alpha_test = 0.01\n",
    "\n",
    "# Проверка функции\n",
    "loss_test, gradient_test = logistic_loss(w_test, X_test, y_test, alpha_test)\n",
    "print(\"Logistic Loss:\", loss_test)\n",
    "print(\"Gradient:\", gradient_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№1. $$\\max_{\\nu\\in\\mathbb{R^m}} -\\frac{1}{m} \\sum\\limits_{i=1}^m \\log\\left(1+\\exp(-u_i)\\right) - \\frac{\\alpha}{2}\\|w\\|_2^2 + \\sum\\limits_{i=1}^m \\nu_i(y_i \\langle w, x_i\\rangle - u_i),$$\n",
    "$$\\text{s.t. } u_i=y_i \\langle w, x_i\\rangle, \\forall i=\\overline{1,m}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№2.\n",
    "\\begin{align*}\n",
    "&\\max_{\\boldsymbol{\\nu}} -\\frac{1}{m} \\sum_{i=1}^{m} \\log(1 + \\exp(-u_i)) - \\frac{\\alpha}{2} \\left\\| \\sum_{i=1}^{m} \\boldsymbol{\\nu}_i y_i \\mathbf{x}_i \\right\\|_2^2 \\\\\n",
    "&+ \\sum_{i=1}^{m} \\boldsymbol{\\nu}_i \\left( y_i \\langle \\alpha \\sum_{j=1}^{m} \\boldsymbol{\\nu}_j y_j \\mathbf{x}_j, \\mathbf{x}_i \\rangle - u_i \\right)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№4. \n",
    "$$\\mu = \\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/task4/X.npy')\n",
    "Y = np.load('data/task4/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (6000,1000) and (3,) not aligned: 1000 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_24064/3884587241.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Тестирование функции\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mt_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbacktracking_line_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgradient_logistic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step Size:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_24064/3884587241.py\u001b[0m in \u001b[0;36mbacktracking_line_search\u001b[1;34m(X, y, w, alpha, gradient, direction, beta1, beta2, max_iter)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mgoldstein_conditions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_24064/3884587241.py\u001b[0m in \u001b[0;36mgoldstein_conditions\u001b[1;34m(X, y, w, alpha, beta1, beta2, t, gradient, direction)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mgrad_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mf_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdir_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_24064/4210130758.py\u001b[0m in \u001b[0;36mlogistic_loss\u001b[1;34m(w, X, y, alpha)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Вычисление отклика модели\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Защита от переполнения\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (6000,1000) and (3,) not aligned: 1000 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# №6\n",
    "\n",
    "def goldstein_conditions(X, y, w, alpha, beta1, beta2, t, gradient, direction):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    grad_w = gradient(w, X, y, alpha)\n",
    "    f_value, _ = logistic_loss(w, X, y, alpha)\n",
    "    dir_w = direction(w, X, y, alpha)\n",
    "    \n",
    "    min_acceptable_value = f_value + beta1 * t * np.dot(grad_w, dir_w)\n",
    "    \n",
    "    expected_decrease = beta2 * t * np.dot(grad_w, dir_w)\n",
    "    \n",
    "    w_new = w - t * dir_w\n",
    "    f_value_new, _ = logistic_loss(w_new, X, y, alpha)\n",
    "    \n",
    "    # Условие Гольдштейна\n",
    "    if f_value_new <= min_acceptable_value:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def backtracking_line_search(X, y, w, alpha, gradient, direction, beta1=0.3, beta2=0.9, max_iter=100):\n",
    "    t = 1.0\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if goldstein_conditions(X, y, w, alpha, beta1, beta2, t, gradient, direction):\n",
    "            break\n",
    "        else:\n",
    "            t *= 0.5\n",
    "    \n",
    "    return t\n",
    "\n",
    "# Тестирование функции\n",
    "t_test = backtracking_line_search(X, Y, w_test, alpha_test, logistic_loss, lambda w, X, y, alpha: -gradient_logistic_loss(w, X, y, alpha), max_iter=100)\n",
    "print(\"Step Size:\", t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
