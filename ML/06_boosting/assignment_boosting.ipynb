{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119c9460",
   "metadata": {},
   "source": [
    "## Home assignment 06: Gradient boosting with MSE\n",
    "\n",
    "Please, fill the lines in the code below.\n",
    "This is a simplified version of `BoostingRegressor` from `sklearn`. Please, notice, that `sklearn` API is **not preserved**.\n",
    "\n",
    "Your algorithm should be able to train different numbers of instances of the same model class. Every target is computed according to the loss function gradient. In this particular case, loss is computed for MSE.\n",
    "\n",
    "The model should be passed as model class with no explicit parameters and no parentheses.\n",
    "\n",
    "Example:\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ecde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06110580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBoostingRegressor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss(targets, predictions):\n",
    "        loss = np.mean((targets - predictions)**2)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_gradients(targets, predictions):\n",
    "        gradients = -2 * (targets - predictions)\n",
    "        assert gradients.shape == targets.shape\n",
    "        return gradients\n",
    "        \n",
    "        \n",
    "    def fit(self, model_constructor, data, targets, num_steps=10, lr=0.1, max_depth=5, verbose=False):\n",
    "        '''\n",
    "        Fit sequence of models on the provided data.\n",
    "        Model constructor with no parameters (and with no ()) is passed to this function.\n",
    "        If \n",
    "        \n",
    "        example:\n",
    "        \n",
    "        boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "        boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "        '''\n",
    "        new_targets = targets\n",
    "        self.models_list = []\n",
    "        self.lr = lr\n",
    "        self.loss_log = []\n",
    "        for step in range(num_steps):\n",
    "            try:\n",
    "                model = model_constructor(max_depth=max_depth)\n",
    "            except TypeError:\n",
    "                print('max_depth keyword is not found. Ignoring')\n",
    "                model = model_constructor()\n",
    "            self.models_list.append(model.fit(data, new_targets))\n",
    "            predictions = self.predict(data)\n",
    "            self.loss_log.append(self.loss(targets, predictions))\n",
    "            gradients = self.loss_gradients(targets, predictions)\n",
    "            new_targets = targets - gradients\n",
    "        if verbose:\n",
    "            print('Finished! Loss=', self.loss_log[-1])\n",
    "        return self\n",
    "            \n",
    "    def predict(self, data):\n",
    "        predictions = np.zeros(len(data))\n",
    "        for model in self.models_list:\n",
    "            predictions += self.lr * model.predict(data) \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa174f",
   "metadata": {},
   "source": [
    "### Local tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54221c2",
   "metadata": {},
   "source": [
    "#### Overfitting tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c94a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9311abda8f14aecbcb9b5288194530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Boosting should overfit with many deep trees on simple data!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_12264/3010421292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mboosting_regressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimplifiedBoostingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mboosting_regressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mboosting_regressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Boosting should overfit with many deep trees on simple data!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mboosting_regressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'First tree loos should be not to low!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Overfitting tests done!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Boosting should overfit with many deep trees on simple data!"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(200, 10)\n",
    "    y = np.random.normal(0, 1, X.shape[0])\n",
    "    boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "    boosting_regressor.fit(DecisionTreeRegressor, X, y, 100, 0.5, 10)\n",
    "    assert boosting_regressor.loss_log[-1] < 1e-6, 'Boosting should overfit with many deep trees on simple data!'\n",
    "    assert boosting_regressor.loss_log[0] > 1e-2, 'First tree loos should be not to low!'    \n",
    "print('Overfitting tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5cfd7",
   "metadata": {},
   "source": [
    "#### Zero lr tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e60fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd17f939eb3642d0967f25a9939b5b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero lr tests done!\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(200, 10)\n",
    "    y = np.random.normal(0, 1, X.shape[0])\n",
    "    boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "    boosting_regressor.fit(DecisionTreeRegressor, X, y, 10, 0., 10)\n",
    "    predictions = boosting_regressor.predict(X)\n",
    "    assert all(predictions == 0), 'With zero weight model should predict constant values!'\n",
    "    assert boosting_regressor.loss_log[-1] == boosting_regressor.loss_log[0], 'With zero weight model should not learn anything new!'\n",
    "print('Zero lr tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2faafe",
   "metadata": {},
   "source": [
    "#### Fitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2710d9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de3b2b66f91460b83e56961c1e729d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "8712.806572316484, 8721.231800638656",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VC4DE~1.DES\\AppData\\Local\\Temp/ipykernel_12264/3655942335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mval_loss_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_val\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtargets_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{}, {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: 8712.806572316484, 8721.231800638656"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    data, targets = make_regression(1000, 10)\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    data_train, targets_train = data[indices[:700]], targets[indices[:700]]\n",
    "    data_val, targets_val = data[indices[700:]], targets[indices[700:]]\n",
    "\n",
    "\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    for depth in range(1, 25):\n",
    "        boosting_regressor = SimplifiedBoostingRegressor()    \n",
    "\n",
    "        boosting_regressor.fit(DecisionTreeRegressor, data_train, targets_train, depth, 0.2, 5)\n",
    "        predictions_train = boosting_regressor.predict(data_train)\n",
    "        predictions_val = boosting_regressor.predict(data_val)\n",
    "        train_loss_log.append(np.mean((predictions_train-targets_train)**2))\n",
    "        val_loss_log.append(np.mean((predictions_val-targets_val)**2))\n",
    "        \n",
    "    assert train_loss_log[-2] > train_loss_log[-1] and abs(train_loss_log[-2]/train_loss_log[-1]) < 2, '{}, {}'.format(train_loss_log[-2], train_loss_log[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedf99c",
   "metadata": {},
   "source": [
    "Here is your convergence plot from the last run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_loss_log)+1), train_loss_log, label='train')\n",
    "plt.plot(range(1, len(val_loss_log)+1), val_loss_log, label='val')\n",
    "plt.xlabel('Ensemble size')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535cb6d",
   "metadata": {},
   "source": [
    "Great job! Please, submit your solution to the grading system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
